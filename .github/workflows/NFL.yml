name: NFL-only props (Underdog)

on:
  workflow_dispatch: {}

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          pip install -q --upgrade pip
          pip install -q pandas requests beautifulsoup4

      - name: Scrape ALL NFL props from Underdog (writes underdog_props_nfl.csv)
        env:
          # optional: set a desktop UA so CDNs return the same payload as the web app
          UA: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36
        run: |
          python - << 'PY'
          import json, re, time, sys
          from datetime import datetime, timezone
          from typing import Dict, List, Any, Tuple
          import requests
          import pandas as pd
          from bs4 import BeautifulSoup

          SESS = requests.Session()
          SESS.headers.update({
              "User-Agent":  os.environ.get("UA","Mozilla/5.0"),
              "Accept":      "application/json,text/html;q=0.9,*/*;q=0.8",
              "Connection":  "keep-alive",
          })

          # ----------------------------- helpers -----------------------------
          def now_iso():
              return datetime.now(timezone.utc).isoformat()

          def jtry(url, params=None) -> Tuple[dict, int]:
              try:
                  r = SESS.get(url, params=params, timeout=20)
                  ct = r.headers.get("Content-Type","")
                  if "application/json" in ct or r.text.strip().startswith("{"):
                      return r.json(), r.status_code
                  # some endpoints return text but are JSON
                  try:
                      return json.loads(r.text), r.status_code
                  except Exception:
                      return {}, r.status_code
              except Exception:
                  return {}, 0

          def is_nfl_text(s: str) -> bool:
              s = (s or "").lower()
              return any(k in s for k in ["nfl", "national football league"])

          def market_is_nfl(record: dict) -> bool:
              # look across multiple hint fields
              hay = " ".join(str(record.get(k,"")) for k in [
                  "sport_name","league","league_name","competition","group","title",
                  "market","category","selection_header","selection_subheader","team","matchup"
              ]).lower()
              return "nfl" in hay

          def norm_float(x):
              try:
                  if x is None: return None
                  if isinstance(x, (int,float)): return float(x)
                  s = str(x).strip().replace(",","")
                  if s.lower() in {"-","nan","none",""}: return None
                  return float(s)
              except Exception:
                  return None

          rows: List[Dict[str,Any]] = []
          sources_log = []

          # ====================== STRATEGY 1: JSON endpoints ======================
          # We try multiple known/observed endpoints the webapp tends to hit.
          # If Underdog rotates an endpoint, we still gather from those that respond.
          json_candidates = [
              # pick'em over/under lines (v3 beta; often live)
              ("api_v3_oul", "https://api.underdogfantasy.com/beta/v3/over_under_lines", {}),
              # sometimes grouped under "pickem" api host
              ("sports_v3_oul", "https://sports.underdogfantasy.com/beta/v3/over_under_lines", {}),
              # market groups (to resolve markets/labels)
              ("api_v3_markets", "https://api.underdogfantasy.com/beta/v3/market_types", {}),
              # slates sometimes expose sport filters
              ("api_v3_slates", "https://api.underdogfantasy.com/beta/v3/slates", {}),
              # legacy odds blob (if present in some regions)
              ("sports_odds_v1", "https://sports.underdogfantasy.com/v1/odds", {}),
          ]

          markets_hint = {}  # id -> market name (if we can enrich later)

          def harvest_over_under_lines(blob: dict):
              count = 0
              # shape 1: {"over_under_lines":[ ... ]}
              lines = blob.get("over_under_lines") or blob.get("lines") or []
              # shape 2: sometimes nested under "data" or "odds"
              if not lines and isinstance(blob.get("data"), dict):
                  lines = blob["data"].get("over_under_lines") or blob["data"].get("lines") or []
              if not lines and isinstance(blob.get("odds"), dict):
                  lines = blob["odds"].get("over_under_lines") or []

              for item in lines:
                  # we try to be defensive about keys
                  sport_name = item.get("sport_name") or item.get("league") or item.get("group") or ""
                  if not (is_nfl_text(sport_name) or market_is_nfl(item)):
                      continue  # keep only NFL
                  # Many shapes store participants & matchups like so:
                  player = item.get("player_name") or item.get("name") or item.get("title") or ""
                  team   = item.get("team") or item.get("team_name") or ""
                  matchup = item.get("matchup") or item.get("game") or item.get("event_name") or ""

                  market = item.get("stat_name") or item.get("market") or item.get("category") or ""
                  line   = norm_float(item.get("stat_value") or item.get("line") or item.get("value"))
                  side   = item.get("choice") or item.get("side") or ""
                  american = item.get("american_price") or item.get("american_odds") or ""
                  decimal  = item.get("decimal_price")  or item.get("decimal_odds")  or ""
                  payout   = item.get("payout_multiplier") or item.get("payout") or ""
                  updated  = item.get("updated_at") or item.get("last_updated") or now_iso()

                  # De-dup key
                  key = (player, market, line, matchup, side)
                  rows.append({
                      "player": player,
                      "team": team,
                      "matchup": matchup,
                      "sport_name": "NFL",
                      "market": market,
                      "line": line,
                      "side": side,
                      "american_price": american,
                      "decimal_price": decimal,
                      "payout_multiplier": payout,
                      "selection_header": item.get("selection_header",""),
                      "selection_subheader": item.get("selection_subheader",""),
                      "updated_at": updated,
                      "source": "json_over_under_lines"
                  })
                  count += 1
              return count

          # Try candidates
          for tag, url, params in json_candidates:
              blob, code = jtry(url, params=params)
              if code and blob:
                  before = len(rows)
                  if tag.endswith("market_types"):
                      # map market id -> readable name (if present)
                      mts = blob.get("market_types") or blob.get("data") or []
                      if isinstance(mts, dict): mts = mts.get("market_types", [])
                      for mt in mts or []:
                          markets_hint[str(mt.get("id"))] = mt.get("name") or mt.get("label")
                  else:
                      cnt = harvest_over_under_lines(blob)
                      sources_log.append((tag, url, code, cnt))
                  after = len(rows)
              else:
                  sources_log.append((tag, url, code, 0))

          # ====================== STRATEGY 2: HTML (__NEXT_DATA__) ======================
          # Fallback path scrapes the public NFL board the web app renders.
          # These pages typically embed a Next.js JSON payload we can parse.
          html_targets = [
              "https://underdogfantasy.com/pick-em/higher-lower/nfl",
              "https://www.underdogfantasy.com/pick-em/higher-lower/nfl",
          ]

          def harvest_next_data(soup):
              # next.js payload is usually in <script id="__NEXT_DATA__">
              script = soup.find("script", id="__NEXT_DATA__")
              if not script or not script.string: 
                  return 0
              try:
                  data = json.loads(script.string)
              except Exception:
                  return 0

              # We look for any props/markets nested in pageProps or props
              payload = data
              for k in ["props","pageProps","__APOLLO_STATE__"]:
                  if isinstance(payload, dict) and k in payload:
                      payload = payload[k]

              text = json.dumps(payload).lower()
              if "nfl" not in text:
                  return 0

              # Heuristic extraction: collect any objects that look like props
              def walk(o):
                  if isinstance(o, dict):
                      yield o
                      for v in o.values():
                          yield from walk(v)
                  elif isinstance(o, list):
                      for v in o:
                          yield from walk(v)

              cnt = 0
              for obj in walk(payload):
                  # detect a likely prop shape
                  if not any(k in obj for k in ["player_name","stat_name","market","line","value","matchup","choice"]):
                      continue
                  if not market_is_nfl(obj):
                      continue

                  player = obj.get("player_name") or obj.get("name") or obj.get("title") or ""
                  market = obj.get("stat_name") or obj.get("market") or obj.get("category") or ""
                  line   = norm_float(obj.get("stat_value") or obj.get("line") or obj.get("value"))
                  side   = obj.get("choice") or obj.get("side") or ""
                  team   = obj.get("team") or ""
                  matchup= obj.get("matchup") or obj.get("game") or ""

                  if not (player and market and line is not None):
                      continue

                  rows.append({
                      "player": player,
                      "team": team,
                      "matchup": matchup,
                      "sport_name": "NFL",
                      "market": market,
                      "line": line,
                      "side": side,
                      "american_price": obj.get("american_price",""),
                      "decimal_price":  obj.get("decimal_price",""),
                      "payout_multiplier": obj.get("payout_multiplier",""),
                      "selection_header": obj.get("selection_header",""),
                      "selection_subheader": obj.get("selection_subheader",""),
                      "updated_at": obj.get("updated_at") or now_iso(),
                      "source": "html_next_data"
                  })
                  cnt += 1
              return cnt

          for url in html_targets:
              try:
                  r = SESS.get(url, timeout=20)
                  soup = BeautifulSoup(r.text, "html.parser")
                  cnt = harvest_next_data(soup)
                  sources_log.append(("html", url, r.status_code, cnt))
              except Exception:
                  sources_log.append(("html", url, 0, 0))

          # ----------------------------- normalize & save -----------------------------
          # Deduplicate rows (same player/market/line/side/matchup)
          if rows:
              df = pd.DataFrame(rows)
              df["key"] = (df["player"].str.strip().str.lower() + "|" +
                           df["market"].str.strip().str.lower() + "|" +
                           df["line"].astype(str) + "|" +
                           df["side"].fillna("").str.strip().str.lower() + "|" +
                           df["matchup"].fillna("").str.strip().str.lower())
              df = df.drop_duplicates("key").drop(columns=["key"])
          else:
              df = pd.DataFrame(columns=[
                  "player","team","matchup","sport_name","market","line","side",
                  "american_price","decimal_price","payout_multiplier",
                  "selection_header","selection_subheader","updated_at","source"
              ])

          df = df.assign(scraped_at_utc=now_iso())
          df.to_csv("underdog_props_nfl.csv", index=False)

          # quick log
          print("=== SOURCES ===")
          for tag,url,code,cnt in sources_log:
              print(f"{tag:18} {code:>3}  {cnt:>5}  {url}")
          print("TOTAL ROWS:", len(df))
          print(df.head(25).to_string(index=False))
          PY

      - name: Upload CSV
        uses: actions/upload-artifact@v4
        with:
          name: underdog_props_nfl
          path: underdog_props_nfl.csv
