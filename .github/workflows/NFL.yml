name: NFL props (Underdog) â€” robust harvest

on:
  workflow_dispatch: {}

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install deps
        run: |
          pip install -q requests pandas tenacity python-dateutil beautifulsoup4

      - name: Scrape ALL NFL props from Underdog (writes underdog_nfl_props.csv + raw payloads)
        run: |
          python - << 'PY'
          import os, json, re, time
          from datetime import datetime, timezone
          from dateutil import parser as dtp
          from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type
          import requests, pandas as pd
          from bs4 import BeautifulSoup

          OUT_CSV = "underdog_nfl_props.csv"
          RAW_DIR = "raw_payloads"
          os.makedirs(RAW_DIR, exist_ok=True)

          UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36"
          HDRS = {
            "User-Agent": UA,
            "Accept": "application/json,text/html;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Referer": "https://underdogfantasy.com/pick-em/higher-lower/nfl",
            "Origin": "https://underdogfantasy.com",
            "Connection": "keep-alive",
          }

          S = requests.Session()
          S.headers.update(HDRS)

          class SoftFail(Exception): ...

          def now_iso(): return datetime.now(timezone.utc).isoformat()

          def save_raw(name: str, content: str|bytes):
              mode = "wb" if isinstance(content, (bytes,bytearray)) else "w"
              with open(os.path.join(RAW_DIR, name), mode) as f:
                  f.write(content)

          @retry(reraise=True,
                 retry=retry_if_exception_type((requests.RequestException, SoftFail)),
                 wait=wait_exponential(multiplier=1, min=1, max=8),
                 stop=stop_after_attempt(5))
          def get(url, params=None, expect_json=True):
              r = S.get(url, params=params, timeout=20)
              if r.status_code in (429,) or r.status_code >= 500:
                  raise SoftFail(f"{r.status_code}")
              r.raise_for_status()
              if expect_json:
                  # sometimes JSON comes as text/plain
                  try:
                      return r.json(), r
                  except Exception:
                      raise SoftFail("not json")
              return r.text, r

          # -------- endpoints: hit many, log/save all --------
          BASES = [
            "https://api.underdogfantasy.com/beta/v3",
            "https://sports.underdogfantasy.com/beta/v3",
            "https://api.underdogfantasy.com/v1"
          ]
          ROUTES = [
            ("over_under_lines", True),
            ("over_under_lines_v2", True),
            ("offers", True),
          ]

          rows = []
          hits = []

          def looks_nfl_bucket(s: str) -> bool:
              s = (s or "").lower()
              return ("nfl" in s) or ("national football league" in s)

          NFL_MARKET_HINTS = [
              "passing", "pass yards", "attempts", "completions", "interceptions", "int",
              "rushing", "rush yards", "rushing attempts", "longest rush",
              "receiving", "receptions", "longest reception",
              "touchdowns", "tds", "fantasy", "field goals", "fg made", "kicking points",
              "tackles", "sacks", "tackles + assists"
          ]
          def looks_nfl_market(s: str) -> bool:
              s = (s or "").lower()
              return any(k in s for k in NFL_MARKET_HINTS)

          def norm_side(s: str) -> str:
              s = (s or "").strip().lower()
              if s in ("higher","over"): return "Over"
              if s in ("lower","under"): return "Under"
              return s.title() if s else ""

          def gf(x):
              try:
                  return float(x)
              except Exception:
                  return None

          def harvest_items(data):
              # accept several shapes
              if isinstance(data, dict):
                  for k in ("over_under_lines","offers","data","items"):
                      if isinstance(data.get(k), list):
                          for it in data[k]:
                              yield it
              elif isinstance(data, list):
                  for it in data:
                      yield it

          def val(obj, *path):
              for p in path:
                  if isinstance(obj, dict) and p in obj:
                      obj = obj[p]
                  else:
                      return None
              return obj

          def as_str(x):
              return (x if isinstance(x,str) else ("" if x is None else str(x))).strip()

          def parse_payload(tag, payload):
              count = 0
              for it in harvest_items(payload):
                  # try to pull common fields
                  appearance = val(it, "appearance") or {}
                  player_obj = val(appearance, "player") or val(it, "player") or {}
                  sport_obj  = val(appearance, "sport") or val(it, "sport") or {}
                  league_obj = val(appearance, "league") or val(it, "league") or {}

                  sport_name  = as_str(val(sport_obj,"name") or val(it,"sport_name"))
                  league_name = as_str(val(league_obj,"name") or val(it,"league_name"))
                  bucket      = " ".join([sport_name, league_name, as_str(val(it,"group"))])

                  player = as_str(val(player_obj,"name") or val(player_obj,"full_name") or val(it,"player_name") or val(it,"name") or val(it,"title"))
                  market = as_str(val(it,"market","name") or val(it,"over_under","stat") or val(it,"stat_type") or val(it,"stat_name") or val(it,"category"))
                  line   = val(it,"line") or val(it,"over_under","line") or val(it,"value") or val(it,"stat_value")
                  side   = norm_side(as_str(val(it,"choice") or val(it,"pick","type") or val(it,"side")))
                  header = as_str(val(it,"selection_header") or val(it,"title") or val(it,"player_header"))
                  sub    = as_str(val(it,"selection_subheader") or val(it,"subtitle") or val(it,"matchup"))
                  upd    = val(it,"updated_at") or val(it,"modified_at") or val(it,"created_at") or ""

                  # NFL gating: either bucket says NFL OR market looks like NFL
                  if not (looks_nfl_bucket(bucket) or looks_nfl_market(market)):
                      continue

                  # require player & market; line can be None for yes/no markets
                  if not (player and market):
                      continue

                  line_num = gf(line)
                  rows.append({
                      "player": player,
                      "sport_name": sport_name or "NFL",
                      "league_name": league_name,
                      "market": market,
                      "line": line_num if line_num is not None else as_str(line),
                      "side": side,
                      "selection_header": header,
                      "selection_subheader": sub,
                      "updated_at": upd,
                      "_source": tag
                  })
                  count += 1
              return count

          # JSON routes with cursor-style pagination
          for base in BASES:
              for route, use_cursor in ROUTES:
                  url = f"{base}/{route}"
                  cursor = None
                  total = 0
                  page = 0
                  while True:
                      params = {"limit": 200}
                      if use_cursor and cursor:
                          params["cursor"] = cursor
                      try:
                          data, resp = get(url, params=params, expect_json=True)
                          save_raw(f"raw_{route}_{page}.json", resp.text.encode("utf-8"))
                          got = parse_payload(f"{base}/{route}", data)
                          total += got
                          # cursor styles
                          cursor = data.get("next_cursor") or data.get("meta",{}).get("next_cursor")
                          page += 1
                          if not use_cursor or not cursor:
                              break
                      except Exception as e:
                          # save error page (if any)
                          break
                  hits.append((url, total))

          # HTML fallback (__NEXT_DATA__)
          HTML_URLS = [
            "https://underdogfantasy.com/pick-em/higher-lower/nfl",
            "https://www.underdogfantasy.com/pick-em/higher-lower/nfl",
          ]
          for u in HTML_URLS:
              try:
                  html, resp = get(u, expect_json=False)
                  save_raw("raw_nfl_board.html", html.encode("utf-8"))
                  soup = BeautifulSoup(html, "html.parser")
                  script = soup.find("script", id="__NEXT_DATA__")
                  if script and script.string:
                      save_raw("raw_next_data.json", script.string.encode("utf-8"))
                      payload = json.loads(script.string)
                      # walk payload for objects that look like props
                      def walk(x):
                          if isinstance(x, dict):
                              yield x
                              for v in x.values(): yield from walk(v)
                          elif isinstance(x, list):
                              for v in x: yield from walk(v)
                      cnt = 0
                      for obj in walk(payload):
                          if not any(k in obj for k in ("player_name","stat_name","market","line","value","matchup","choice")):
                              continue
                          # wrap and reuse parser
                          cnt += parse_payload(f"{u}#__NEXT_DATA__", obj)
                      hits.append((u, cnt))
              except Exception:
                  hits.append((u, 0))

          # Build dataframe, dedupe
          df = pd.DataFrame(rows)
          if not df.empty:
              def parse_dt(x):
                  try: return dtp.parse(str(x))
                  except: return datetime.min.replace(tzinfo=timezone.utc)
              df["_ts"] = df["updated_at"].apply(parse_dt)
              df["_key"] = (df["player"].str.lower().str.strip()+"|"+
                            df["market"].str.lower().str.strip()+"|"+
                            df["line"].astype(str)+"|"+
                            df["side"].str.lower().str.strip())
              df = (df.sort_values("_ts", ascending=False)
                      .drop_duplicates("_key")
                      .drop(columns=["_ts","_key"]))
          df.to_csv(OUT_CSV, index=False)

          print("SOURCES & COUNTS:")
          for (u,c) in hits:
              print(f"{c:5d}  {u}")
          print("TOTAL ROWS:", 0 if df.empty else len(df))
          PY

      - name: Upload outputs
        uses: actions/upload-artifact@v4
        with:
          name: underdog_nfl_dump
          path: |
            underdog_nfl_props.csv
            raw_payloads/**
